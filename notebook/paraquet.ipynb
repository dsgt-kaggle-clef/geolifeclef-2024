{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BinaryType\n",
    "import rasterio\n",
    "from rasterio.windows import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tiled_tiff(tiff_path):\n",
    "  with rasterio.open(tiff_path) as src:\n",
    "    print(src.meta)\n",
    "    for band in range(src.meta['count']):\n",
    "      for _, window in src.block_windows(band):\n",
    "        data = src.read(window=window)\n",
    "        binary_data = data.tobytes()\n",
    "\n",
    "        tile_x, tile_y = window.col_off, window.row_off\n",
    "        tile_x, tile_y = int(tile_x), int(tile_y)\n",
    "        yield tile_x, tile_y, binary_data, (band + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'uint16', 'nodata': None, 'width': 8143, 'height': 5467, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.0083333333, 0.0, -32.266806146449994,\n",
      "       0.0, -0.0083333333, 72.19152712905)}\n",
      "0 0 1\n",
      "256 0 1\n",
      "512 0 1\n",
      "768 0 1\n",
      "1024 0 1\n",
      "1280 0 1\n",
      "1536 0 1\n",
      "1792 0 1\n",
      "2048 0 1\n",
      "2304 0 1\n",
      "2560 0 1\n",
      "2816 0 1\n",
      "3072 0 1\n",
      "3328 0 1\n",
      "3584 0 1\n",
      "3840 0 1\n",
      "4096 0 1\n",
      "4352 0 1\n",
      "4608 0 1\n",
      "4864 0 1\n",
      "5120 0 1\n",
      "5376 0 1\n",
      "5632 0 1\n",
      "5888 0 1\n",
      "6144 0 1\n",
      "6400 0 1\n",
      "6656 0 1\n",
      "6912 0 1\n",
      "7168 0 1\n",
      "7424 0 1\n",
      "7680 0 1\n",
      "7936 0 1\n",
      "0 256 1\n",
      "256 256 1\n",
      "512 256 1\n",
      "768 256 1\n",
      "1024 256 1\n",
      "1280 256 1\n",
      "1536 256 1\n",
      "1792 256 1\n",
      "2048 256 1\n",
      "2304 256 1\n",
      "2560 256 1\n",
      "2816 256 1\n",
      "3072 256 1\n",
      "3328 256 1\n",
      "3584 256 1\n",
      "3840 256 1\n",
      "4096 256 1\n",
      "4352 256 1\n",
      "4608 256 1\n",
      "4864 256 1\n",
      "5120 256 1\n",
      "5376 256 1\n",
      "5632 256 1\n",
      "5888 256 1\n",
      "6144 256 1\n",
      "6400 256 1\n",
      "6656 256 1\n",
      "6912 256 1\n",
      "7168 256 1\n",
      "7424 256 1\n",
      "7680 256 1\n",
      "7936 256 1\n",
      "0 512 1\n",
      "256 512 1\n",
      "512 512 1\n",
      "768 512 1\n",
      "1024 512 1\n",
      "1280 512 1\n",
      "1536 512 1\n",
      "1792 512 1\n",
      "2048 512 1\n",
      "2304 512 1\n",
      "2560 512 1\n",
      "2816 512 1\n",
      "3072 512 1\n",
      "3328 512 1\n",
      "3584 512 1\n",
      "3840 512 1\n",
      "4096 512 1\n",
      "4352 512 1\n",
      "4608 512 1\n",
      "4864 512 1\n",
      "5120 512 1\n",
      "5376 512 1\n",
      "5632 512 1\n",
      "5888 512 1\n",
      "6144 512 1\n",
      "6400 512 1\n",
      "6656 512 1\n",
      "6912 512 1\n",
      "7168 512 1\n",
      "7424 512 1\n",
      "7680 512 1\n",
      "7936 512 1\n",
      "0 768 1\n",
      "256 768 1\n",
      "512 768 1\n",
      "768 768 1\n",
      "1024 768 1\n",
      "1280 768 1\n",
      "1536 768 1\n",
      "1792 768 1\n",
      "2048 768 1\n",
      "2304 768 1\n",
      "2560 768 1\n",
      "2816 768 1\n",
      "3072 768 1\n",
      "3328 768 1\n",
      "3584 768 1\n",
      "3840 768 1\n",
      "4096 768 1\n",
      "4352 768 1\n",
      "4608 768 1\n",
      "4864 768 1\n",
      "5120 768 1\n",
      "5376 768 1\n",
      "5632 768 1\n",
      "5888 768 1\n",
      "6144 768 1\n",
      "6400 768 1\n",
      "6656 768 1\n",
      "6912 768 1\n",
      "7168 768 1\n",
      "7424 768 1\n",
      "7680 768 1\n",
      "7936 768 1\n",
      "0 1024 1\n",
      "256 1024 1\n",
      "512 1024 1\n",
      "768 1024 1\n",
      "1024 1024 1\n",
      "1280 1024 1\n",
      "1536 1024 1\n",
      "1792 1024 1\n",
      "2048 1024 1\n",
      "2304 1024 1\n",
      "2560 1024 1\n",
      "2816 1024 1\n",
      "3072 1024 1\n",
      "3328 1024 1\n",
      "3584 1024 1\n",
      "3840 1024 1\n",
      "4096 1024 1\n",
      "4352 1024 1\n",
      "4608 1024 1\n",
      "4864 1024 1\n",
      "5120 1024 1\n",
      "5376 1024 1\n",
      "5632 1024 1\n",
      "5888 1024 1\n",
      "6144 1024 1\n",
      "6400 1024 1\n",
      "6656 1024 1\n",
      "6912 1024 1\n",
      "7168 1024 1\n",
      "7424 1024 1\n",
      "7680 1024 1\n",
      "7936 1024 1\n",
      "0 1280 1\n",
      "256 1280 1\n",
      "512 1280 1\n",
      "768 1280 1\n",
      "1024 1280 1\n",
      "1280 1280 1\n",
      "1536 1280 1\n",
      "1792 1280 1\n",
      "2048 1280 1\n",
      "2304 1280 1\n",
      "2560 1280 1\n",
      "2816 1280 1\n",
      "3072 1280 1\n",
      "3328 1280 1\n",
      "3584 1280 1\n",
      "3840 1280 1\n",
      "4096 1280 1\n",
      "4352 1280 1\n",
      "4608 1280 1\n",
      "4864 1280 1\n",
      "5120 1280 1\n",
      "5376 1280 1\n",
      "5632 1280 1\n",
      "5888 1280 1\n",
      "6144 1280 1\n",
      "6400 1280 1\n",
      "6656 1280 1\n",
      "6912 1280 1\n",
      "7168 1280 1\n",
      "7424 1280 1\n",
      "7680 1280 1\n",
      "7936 1280 1\n",
      "0 1536 1\n",
      "256 1536 1\n",
      "512 1536 1\n",
      "768 1536 1\n",
      "1024 1536 1\n",
      "1280 1536 1\n",
      "1536 1536 1\n",
      "1792 1536 1\n",
      "2048 1536 1\n",
      "2304 1536 1\n",
      "2560 1536 1\n",
      "2816 1536 1\n",
      "3072 1536 1\n",
      "3328 1536 1\n",
      "3584 1536 1\n",
      "3840 1536 1\n",
      "4096 1536 1\n",
      "4352 1536 1\n",
      "4608 1536 1\n",
      "4864 1536 1\n",
      "5120 1536 1\n",
      "5376 1536 1\n",
      "5632 1536 1\n",
      "5888 1536 1\n",
      "6144 1536 1\n",
      "6400 1536 1\n",
      "6656 1536 1\n",
      "6912 1536 1\n",
      "7168 1536 1\n",
      "7424 1536 1\n",
      "7680 1536 1\n",
      "7936 1536 1\n",
      "0 1792 1\n",
      "256 1792 1\n",
      "512 1792 1\n",
      "768 1792 1\n",
      "1024 1792 1\n",
      "1280 1792 1\n",
      "1536 1792 1\n",
      "1792 1792 1\n",
      "2048 1792 1\n",
      "2304 1792 1\n",
      "2560 1792 1\n",
      "2816 1792 1\n",
      "3072 1792 1\n",
      "3328 1792 1\n",
      "3584 1792 1\n",
      "3840 1792 1\n",
      "4096 1792 1\n",
      "4352 1792 1\n",
      "4608 1792 1\n",
      "4864 1792 1\n",
      "5120 1792 1\n",
      "5376 1792 1\n",
      "5632 1792 1\n",
      "5888 1792 1\n",
      "6144 1792 1\n",
      "6400 1792 1\n",
      "6656 1792 1\n",
      "6912 1792 1\n",
      "7168 1792 1\n",
      "7424 1792 1\n",
      "7680 1792 1\n",
      "7936 1792 1\n",
      "0 2048 1\n",
      "256 2048 1\n",
      "512 2048 1\n",
      "768 2048 1\n",
      "1024 2048 1\n",
      "1280 2048 1\n",
      "1536 2048 1\n",
      "1792 2048 1\n",
      "2048 2048 1\n",
      "2304 2048 1\n",
      "2560 2048 1\n",
      "2816 2048 1\n",
      "3072 2048 1\n",
      "3328 2048 1\n",
      "3584 2048 1\n",
      "3840 2048 1\n",
      "4096 2048 1\n",
      "4352 2048 1\n",
      "4608 2048 1\n",
      "4864 2048 1\n",
      "5120 2048 1\n",
      "5376 2048 1\n",
      "5632 2048 1\n",
      "5888 2048 1\n",
      "6144 2048 1\n",
      "6400 2048 1\n",
      "6656 2048 1\n",
      "6912 2048 1\n",
      "7168 2048 1\n",
      "7424 2048 1\n",
      "7680 2048 1\n",
      "7936 2048 1\n",
      "0 2304 1\n",
      "256 2304 1\n",
      "512 2304 1\n",
      "768 2304 1\n",
      "1024 2304 1\n",
      "1280 2304 1\n",
      "1536 2304 1\n",
      "1792 2304 1\n",
      "2048 2304 1\n",
      "2304 2304 1\n",
      "2560 2304 1\n",
      "2816 2304 1\n",
      "3072 2304 1\n",
      "3328 2304 1\n",
      "3584 2304 1\n",
      "3840 2304 1\n",
      "4096 2304 1\n",
      "4352 2304 1\n",
      "4608 2304 1\n",
      "4864 2304 1\n",
      "5120 2304 1\n",
      "5376 2304 1\n",
      "5632 2304 1\n",
      "5888 2304 1\n",
      "6144 2304 1\n",
      "6400 2304 1\n",
      "6656 2304 1\n",
      "6912 2304 1\n",
      "7168 2304 1\n",
      "7424 2304 1\n",
      "7680 2304 1\n",
      "7936 2304 1\n",
      "0 2560 1\n",
      "256 2560 1\n",
      "512 2560 1\n",
      "768 2560 1\n",
      "1024 2560 1\n",
      "1280 2560 1\n",
      "1536 2560 1\n",
      "1792 2560 1\n",
      "2048 2560 1\n",
      "2304 2560 1\n",
      "2560 2560 1\n",
      "2816 2560 1\n",
      "3072 2560 1\n",
      "3328 2560 1\n",
      "3584 2560 1\n",
      "3840 2560 1\n",
      "4096 2560 1\n",
      "4352 2560 1\n",
      "4608 2560 1\n",
      "4864 2560 1\n",
      "5120 2560 1\n",
      "5376 2560 1\n",
      "5632 2560 1\n",
      "5888 2560 1\n",
      "6144 2560 1\n",
      "6400 2560 1\n",
      "6656 2560 1\n",
      "6912 2560 1\n",
      "7168 2560 1\n",
      "7424 2560 1\n",
      "7680 2560 1\n",
      "7936 2560 1\n",
      "0 2816 1\n",
      "256 2816 1\n",
      "512 2816 1\n",
      "768 2816 1\n",
      "1024 2816 1\n",
      "1280 2816 1\n",
      "1536 2816 1\n",
      "1792 2816 1\n",
      "2048 2816 1\n",
      "2304 2816 1\n",
      "2560 2816 1\n",
      "2816 2816 1\n",
      "3072 2816 1\n",
      "3328 2816 1\n",
      "3584 2816 1\n",
      "3840 2816 1\n",
      "4096 2816 1\n",
      "4352 2816 1\n",
      "4608 2816 1\n",
      "4864 2816 1\n",
      "5120 2816 1\n",
      "5376 2816 1\n",
      "5632 2816 1\n",
      "5888 2816 1\n",
      "6144 2816 1\n",
      "6400 2816 1\n",
      "6656 2816 1\n",
      "6912 2816 1\n",
      "7168 2816 1\n",
      "7424 2816 1\n",
      "7680 2816 1\n",
      "7936 2816 1\n",
      "0 3072 1\n",
      "256 3072 1\n",
      "512 3072 1\n",
      "768 3072 1\n",
      "1024 3072 1\n",
      "1280 3072 1\n",
      "1536 3072 1\n",
      "1792 3072 1\n",
      "2048 3072 1\n",
      "2304 3072 1\n",
      "2560 3072 1\n",
      "2816 3072 1\n",
      "3072 3072 1\n",
      "3328 3072 1\n",
      "3584 3072 1\n",
      "3840 3072 1\n",
      "4096 3072 1\n",
      "4352 3072 1\n",
      "4608 3072 1\n",
      "4864 3072 1\n",
      "5120 3072 1\n",
      "5376 3072 1\n",
      "5632 3072 1\n",
      "5888 3072 1\n",
      "6144 3072 1\n",
      "6400 3072 1\n",
      "6656 3072 1\n",
      "6912 3072 1\n",
      "7168 3072 1\n",
      "7424 3072 1\n",
      "7680 3072 1\n",
      "7936 3072 1\n",
      "0 3328 1\n",
      "256 3328 1\n",
      "512 3328 1\n",
      "768 3328 1\n",
      "1024 3328 1\n",
      "1280 3328 1\n",
      "1536 3328 1\n",
      "1792 3328 1\n",
      "2048 3328 1\n",
      "2304 3328 1\n",
      "2560 3328 1\n",
      "2816 3328 1\n",
      "3072 3328 1\n",
      "3328 3328 1\n",
      "3584 3328 1\n",
      "3840 3328 1\n",
      "4096 3328 1\n",
      "4352 3328 1\n",
      "4608 3328 1\n",
      "4864 3328 1\n",
      "5120 3328 1\n",
      "5376 3328 1\n",
      "5632 3328 1\n",
      "5888 3328 1\n",
      "6144 3328 1\n",
      "6400 3328 1\n",
      "6656 3328 1\n",
      "6912 3328 1\n",
      "7168 3328 1\n",
      "7424 3328 1\n",
      "7680 3328 1\n",
      "7936 3328 1\n",
      "0 3584 1\n",
      "256 3584 1\n",
      "512 3584 1\n",
      "768 3584 1\n",
      "1024 3584 1\n",
      "1280 3584 1\n",
      "1536 3584 1\n",
      "1792 3584 1\n",
      "2048 3584 1\n",
      "2304 3584 1\n",
      "2560 3584 1\n",
      "2816 3584 1\n",
      "3072 3584 1\n",
      "3328 3584 1\n",
      "3584 3584 1\n",
      "3840 3584 1\n",
      "4096 3584 1\n",
      "4352 3584 1\n",
      "4608 3584 1\n",
      "4864 3584 1\n",
      "5120 3584 1\n",
      "5376 3584 1\n",
      "5632 3584 1\n",
      "5888 3584 1\n",
      "6144 3584 1\n",
      "6400 3584 1\n",
      "6656 3584 1\n",
      "6912 3584 1\n",
      "7168 3584 1\n",
      "7424 3584 1\n",
      "7680 3584 1\n",
      "7936 3584 1\n",
      "0 3840 1\n",
      "256 3840 1\n",
      "512 3840 1\n",
      "768 3840 1\n",
      "1024 3840 1\n",
      "1280 3840 1\n",
      "1536 3840 1\n",
      "1792 3840 1\n",
      "2048 3840 1\n",
      "2304 3840 1\n",
      "2560 3840 1\n",
      "2816 3840 1\n",
      "3072 3840 1\n",
      "3328 3840 1\n",
      "3584 3840 1\n",
      "3840 3840 1\n",
      "4096 3840 1\n",
      "4352 3840 1\n",
      "4608 3840 1\n",
      "4864 3840 1\n",
      "5120 3840 1\n",
      "5376 3840 1\n",
      "5632 3840 1\n",
      "5888 3840 1\n",
      "6144 3840 1\n",
      "6400 3840 1\n",
      "6656 3840 1\n",
      "6912 3840 1\n",
      "7168 3840 1\n",
      "7424 3840 1\n",
      "7680 3840 1\n",
      "7936 3840 1\n",
      "0 4096 1\n",
      "256 4096 1\n",
      "512 4096 1\n",
      "768 4096 1\n",
      "1024 4096 1\n",
      "1280 4096 1\n",
      "1536 4096 1\n",
      "1792 4096 1\n",
      "2048 4096 1\n",
      "2304 4096 1\n",
      "2560 4096 1\n",
      "2816 4096 1\n",
      "3072 4096 1\n",
      "3328 4096 1\n",
      "3584 4096 1\n",
      "3840 4096 1\n",
      "4096 4096 1\n",
      "4352 4096 1\n",
      "4608 4096 1\n",
      "4864 4096 1\n",
      "5120 4096 1\n",
      "5376 4096 1\n",
      "5632 4096 1\n",
      "5888 4096 1\n",
      "6144 4096 1\n",
      "6400 4096 1\n",
      "6656 4096 1\n",
      "6912 4096 1\n",
      "7168 4096 1\n",
      "7424 4096 1\n",
      "7680 4096 1\n",
      "7936 4096 1\n",
      "0 4352 1\n",
      "256 4352 1\n",
      "512 4352 1\n",
      "768 4352 1\n",
      "1024 4352 1\n",
      "1280 4352 1\n",
      "1536 4352 1\n",
      "1792 4352 1\n",
      "2048 4352 1\n",
      "2304 4352 1\n",
      "2560 4352 1\n",
      "2816 4352 1\n",
      "3072 4352 1\n",
      "3328 4352 1\n",
      "3584 4352 1\n",
      "3840 4352 1\n",
      "4096 4352 1\n",
      "4352 4352 1\n",
      "4608 4352 1\n",
      "4864 4352 1\n",
      "5120 4352 1\n",
      "5376 4352 1\n",
      "5632 4352 1\n",
      "5888 4352 1\n",
      "6144 4352 1\n",
      "6400 4352 1\n",
      "6656 4352 1\n",
      "6912 4352 1\n",
      "7168 4352 1\n",
      "7424 4352 1\n",
      "7680 4352 1\n",
      "7936 4352 1\n",
      "0 4608 1\n",
      "256 4608 1\n",
      "512 4608 1\n",
      "768 4608 1\n",
      "1024 4608 1\n",
      "1280 4608 1\n",
      "1536 4608 1\n",
      "1792 4608 1\n",
      "2048 4608 1\n",
      "2304 4608 1\n",
      "2560 4608 1\n",
      "2816 4608 1\n",
      "3072 4608 1\n",
      "3328 4608 1\n",
      "3584 4608 1\n",
      "3840 4608 1\n",
      "4096 4608 1\n",
      "4352 4608 1\n",
      "4608 4608 1\n",
      "4864 4608 1\n",
      "5120 4608 1\n",
      "5376 4608 1\n",
      "5632 4608 1\n",
      "5888 4608 1\n",
      "6144 4608 1\n",
      "6400 4608 1\n",
      "6656 4608 1\n",
      "6912 4608 1\n",
      "7168 4608 1\n",
      "7424 4608 1\n",
      "7680 4608 1\n",
      "7936 4608 1\n",
      "0 4864 1\n",
      "256 4864 1\n",
      "512 4864 1\n",
      "768 4864 1\n",
      "1024 4864 1\n",
      "1280 4864 1\n",
      "1536 4864 1\n",
      "1792 4864 1\n",
      "2048 4864 1\n",
      "2304 4864 1\n",
      "2560 4864 1\n",
      "2816 4864 1\n",
      "3072 4864 1\n",
      "3328 4864 1\n",
      "3584 4864 1\n",
      "3840 4864 1\n",
      "4096 4864 1\n",
      "4352 4864 1\n",
      "4608 4864 1\n",
      "4864 4864 1\n",
      "5120 4864 1\n",
      "5376 4864 1\n",
      "5632 4864 1\n",
      "5888 4864 1\n",
      "6144 4864 1\n",
      "6400 4864 1\n",
      "6656 4864 1\n",
      "6912 4864 1\n",
      "7168 4864 1\n",
      "7424 4864 1\n",
      "7680 4864 1\n",
      "7936 4864 1\n",
      "0 5120 1\n",
      "256 5120 1\n",
      "512 5120 1\n",
      "768 5120 1\n",
      "1024 5120 1\n",
      "1280 5120 1\n",
      "1536 5120 1\n",
      "1792 5120 1\n",
      "2048 5120 1\n",
      "2304 5120 1\n",
      "2560 5120 1\n",
      "2816 5120 1\n",
      "3072 5120 1\n",
      "3328 5120 1\n",
      "3584 5120 1\n",
      "3840 5120 1\n",
      "4096 5120 1\n",
      "4352 5120 1\n",
      "4608 5120 1\n",
      "4864 5120 1\n",
      "5120 5120 1\n",
      "5376 5120 1\n",
      "5632 5120 1\n",
      "5888 5120 1\n",
      "6144 5120 1\n",
      "6400 5120 1\n",
      "6656 5120 1\n",
      "6912 5120 1\n",
      "7168 5120 1\n",
      "7424 5120 1\n",
      "7680 5120 1\n",
      "7936 5120 1\n",
      "0 5376 1\n",
      "256 5376 1\n",
      "512 5376 1\n",
      "768 5376 1\n",
      "1024 5376 1\n",
      "1280 5376 1\n",
      "1536 5376 1\n",
      "1792 5376 1\n",
      "2048 5376 1\n",
      "2304 5376 1\n",
      "2560 5376 1\n",
      "2816 5376 1\n",
      "3072 5376 1\n",
      "3328 5376 1\n",
      "3584 5376 1\n",
      "3840 5376 1\n",
      "4096 5376 1\n",
      "4352 5376 1\n",
      "4608 5376 1\n",
      "4864 5376 1\n",
      "5120 5376 1\n",
      "5376 5376 1\n",
      "5632 5376 1\n",
      "5888 5376 1\n",
      "6144 5376 1\n",
      "6400 5376 1\n",
      "6656 5376 1\n",
      "6912 5376 1\n",
      "7168 5376 1\n",
      "7424 5376 1\n",
      "7680 5376 1\n",
      "7936 5376 1\n"
     ]
    }
   ],
   "source": [
    "for x, y, b, band in read_tiled_tiff('/mnt/data/processed/bio1_tiled.tif'):\n",
    "  print(x, y,band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver': 'GTiff',\n",
       " 'dtype': 'uint16',\n",
       " 'nodata': None,\n",
       " 'width': 8143,\n",
       " 'height': 5467,\n",
       " 'count': 1,\n",
       " 'crs': CRS.from_epsg(4326),\n",
       " 'transform': Affine(0.0083333333, 0.0, -32.266806146449994,\n",
       "        0.0, -0.0083333333, 72.19152712905)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'uint16', 'nodata': None, 'width': 8143, 'height': 5467, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.0083333333, 0.0, -32.266806146449994,\n",
      "       0.0, -0.0083333333, 72.19152712905)}\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `bio1_tiled.tif` in type `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y, data, band \u001b[38;5;129;01min\u001b[39;00m read_tiled_tiff(path):\n\u001b[1;32m     20\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(Row(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, band\u001b[38;5;241m=\u001b[39mband, filename\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path), binary_data\u001b[38;5;241m=\u001b[39mdata))\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             },\n\u001b[1;32m   2158\u001b[0m         )\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py:2076\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_integer\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2075\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2076\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2077\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2147483648\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2147483647\u001b[39m:\n\u001b[1;32m   2078\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2079\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_OUT_OF_BOUND\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2080\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2085\u001b[0m             },\n\u001b[1;32m   2086\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2012\u001b[0m             },\n\u001b[1;32m   2013\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `bio1_tiled.tif` in type `str`."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"x\", IntegerType(), True),\n",
    "    StructField(\"y\", IntegerType(), True),\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"band\", IntegerType(), True),\n",
    "    StructField(\"binary_data\", BinaryType(), True)\n",
    "])\n",
    "\n",
    "# Assuming you have a list of TIFF paths\n",
    "tiff_paths = ['/mnt/data/processed/bio1_tiled.tif']\n",
    "rows = []\n",
    "\n",
    "for path in tiff_paths:\n",
    "    for x, y, data, band in read_tiled_tiff(path):\n",
    "        \n",
    "        rows.append(Row(x=x, y=y, filename=os.path.basename(path), band=band, binary_data=data))\n",
    "\n",
    "df = spark.createDataFrame(rows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 19:42:21 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/02/10 19:42:21 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (10.3.58.11 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "24/02/10 19:42:21 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (10.3.58.11 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/geolife/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (10.3.58.11 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2076, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/home/elmo/anaconda3/envs/geolife/lib/python3.9/site-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `x` in type `str`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geolife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
